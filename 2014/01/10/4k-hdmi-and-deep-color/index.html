<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    
    <title>4K, HDMI, and Deep Color | TooNormal</title>
    
    <meta name="description" content="As of this writing (January 2014), there are 2 HDMI specifications that support 4K Video (3840√ó2160 16:9). HDMI 1.4 and HDMI 2.0. As far as I know, there are currently no HDMI 2.0 capable TVs available in the market (though many were announced at CES this week).
A detail that tends to be neglected in all this 4K buzz is the Chroma Subsampling. If you‚Äôve ever compared an HDMI signal against something else (DVI, VGA), and the quality looked worse, one of the reasons is because of Chroma Subsampling (for the other reason, see xvYCC at the bottom of this post).">
    <meta name="author" content="Mike K">
    
    <link href="https://toonormal.com/an-old-hope.min.css" rel="stylesheet">
    <link href="https://toonormal.com/style.css" rel="stylesheet">
    <link href="https://toonormal.com/custom.css" rel="stylesheet">
    
    <link rel="apple-touch-icon" href="https://toonormal.com/apple-touch-icon.png">
    <link rel="icon" href="https://toonormal.com/favicon.ico">
    <meta name="generator" content="Hugo 0.74.1" />
    
    
    
    <script>
      function setTheme() {
      	

      }
    </script>
  </head>
  <body class="single">
    <script>
      setTheme();
    </script>
    <header class="header">
      <nav class="nav">
        <span class="logo">
        	<a href="https://toonormal.com/">TooNormal</a>
			<span class="subtitle"> - a note-blog by Mike Kasprzak</span>
        </span>
        <ul class="menu">
          <li>
            <a href="/about/">About</a>
          </li>
          <li>
            <a href="/blog/">Notes &amp; Blog</a>
          </li>
          <li>
            <a href="/recipes/">Recipes</a>
          </li>
          <li>
            <a href="https://twitter.com/mikekasprzak">Twitter</a>
          </li>
          <li>
            <a href="https://youtube.com/mikekasprzak">YouTube</a>
          </li>
        </ul>
      </nav>
    </header>
    <main class="main">


<article class="post-single">
  <header class="post-header">
    <h1 class="post-title">4K, HDMI, and Deep Color</h1>
    <div class="post-meta">Mike K ¬∑ January 10, 2014
    </div>
  </header>
  <div class="post-content"><p>As of this writing (January 2014), there are 2 HDMI specifications that support 4K Video (3840√ó2160 16:9). HDMI 1.4 and HDMI 2.0. As far as I know, there are currently no HDMI 2.0 capable TVs available in the market (though many were announced at CES this week).</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>A detail that tends to be neglected in all this 4K buzz is the Chroma Subsampling. If you‚Äôve ever compared an HDMI signal against something else (DVI, VGA), and the quality looked worse, one of the reasons is because of Chroma Subsampling (for the other reason, see <strong>xvYCC</strong> at the bottom of this post).</p>
<p>Chroma Subsampling is extremely common. Practically every video you‚Äôve ever watched on a computer or other digital video player uses it. As does the JPEG file format. That‚Äôs why we GameDevs prefer formats like PNG that don‚Äôt subsample. We like our source data raw and pristine. We can ruin it later with subsampling or other forms of texture compression (DXT/S3TC).</p>
<p>In the land of Subsampling, a descriptor like <strong>4:4:4</strong> or <strong>4:2:2</strong> is used. Images are broken up in to 4√ó2 pixel cells. The descriptor says how much color (chroma) data is lost. <strong>4:4:4</strong> is the perfect form of Chroma Subsampling. Chroma Subsampling uses the <strong>YCbCr</strong> color space (sometimes called YCC) as opposed to the standard RGB color space.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Occasionally the term ‚Äú<strong>4:4:4 RGB</strong>‚Äù or just ‚Äú<strong>RGB</strong>‚Äù is used to describe the standard RGB color space. Also note, Component Video cables, though they are colored red, green, and blue, are actually <!-- raw HTML omitted -->YPbPr<!-- raw HTML omitted --> encoded (the Analog version of YCbCr).</p>
<p>Looking at the first diagram again, we can make a little more sense of it.</p>
<!-- raw HTML omitted -->
<p>In other words:</p>
<ul>
<li>HDMI 1.4 supports <strong>8bit RGB</strong>, <strong>8bit 4:4:4</strong> YCbCr, and <strong>12bit 4:2:2</strong> YCbCr, all at 24-30 FPS</li>
<li>HDMI 2.0 supports <strong>RGB</strong> and <strong>4:4:4</strong> in all color depths (8bit-16bit) at 24-30 FPS</li>
<li>HDMI 2.0 only supports <strong>8bit RGB</strong> and <strong>8bit 4:4:4</strong> at 60 FPS</li>
<li>All other color depths require Chroma Subsampling at 60 FPS in HDMI 2.0</li>
<li>Peter Jackson‚Äôs 48 FPS (The Hobbit‚Äôs ‚ÄúHigh Frame Rate‚Äù HFR) is notably absent from the spec
<!-- raw HTML omitted --> <!-- raw HTML omitted --></li>
</ul>
<p>Also worth noting, the most well supported color depths are <strong>8bit</strong> and <strong>12bit</strong>. The 12 bit over HDMI is referred to as <strong>Deep Color</strong> (as opposed to High Color).</p>
<p>The HDMI spec has supported only <strong>4:4:4</strong> and <strong>4:2:2</strong> since HDMI 1.0. As of HDMI 2.0, it also supports <strong>4:2:0</strong>, which is available in HDMI 2.0‚Äôs 60 FPS framerates. Blu-ray movies are encoded in 4:2:0, so I‚Äôd assume this is why they added this.</p>
<p>All this video signal butchering does beg the question: Which is the better trade off? More color range per pixel, or more pixels with color channels?</p>
<p>I have no idea.</p>
<p>If I was to guess though, because TV‚Äôs aren‚Äôt right in front of your face like a Computer Monitor, I‚Äôd expect 4K 4:2:2 might actually be better. Greater luminance precision, with a bit of chroma fringing.</p>
<p>Some Plasma and LCD screens use something called <a href="http://en.wikipedia.org/wiki/PenTile_matrix_family">Pentile Matrix</a> arrangement of their red, green, and blue pixels.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>So even if we wanted more color fidelity per individual pixel, it may not be physically there.</p>
<h2 id="deep-color">Deep Color</h2>
<p>Me, my latest graphics fascination is <strong>Deep Color</strong>. Deep Color is the marketing name for more than 8 bits per pixel of a color. It isn‚Äôt necessarily something we need in asset creation (not me, but some do want full 16bit color channels). But as we start running filters/shaders on our assets, stuff like HDR (but more than that), we end up losing the quality of the original assets as they are re-sampled to fit in to an 8bit RGB color space.</p>
<p>This can result in banding, especially in near flat color gradients.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Photographers have RAW and HDR file formats for dealing with this stuff. We have Deep Color, in all its <strong>30bit</strong> (10bpp), <strong>36bit</strong> (12bpp) and <strong>48bit</strong> (16bpp) glory. Or really, just <strong>36bit (12bpp)</strong>, but 48bit can be used as a RAW format if we wanted.</p>
<p>So the point of this nerding: An ideal display would be <strong>4K</strong>, support <strong>12bit RGB</strong> or <strong>12bit YCbCr</strong>, at <strong>60 FPS</strong>.</p>
<p>The thing is, <strong>HDMI 2.0 doesn‚Äôt support it</strong>!</p>
<p>Perhaps that‚Äôs fine though. Again, HDMI is a television spec. Most television viewers are watching video, and practically all video is 4:2:0 encoded anyway (which is supported by the HDMI 2.0 spec). The problem is gaming, where our framerates can reach 60FPS.</p>
<p><strong>The HDMI 2.0 spec isn‚Äôt up to spec.</strong> üòâ</p>
<p>Again this is probably fine. The now-current generation of consoles, nobody is really pushing them as 4K machines anyway. Sony may have 4K video playback support, but most high end games are still targeting 1080p and even 720p. 4K is 4x the pixels of 1080p. I suppose it‚Äôs an advantage that 4K only supports 30FPS right now, meaning you only need to push 2x the data to be a ‚Äú4K game‚Äù, but still.</p>
<p>HDMI Bandwidth is rated in <a href="http://en.wikipedia.org/wiki/HDMI#Version_comparison">Gigabits per second</a>.</p>
<ul>
<li>HDMI 1.0-&gt;1.2: <strong>~4 Gb</strong></li>
<li>HDMI 1.3-&gt;1.4: <strong>~8 Gb</strong></li>
<li>HDMI 2.0: <strong>~14 Gb</strong> (NEW)</li>
</ul>
<p>Not surprisingly, 4K 8bit 60FPS is <strong>~12 Gb</strong> of data, and 30FPS is <strong>~6 Gb</strong> of data. Our good friend <strong>4K 12bit 60FPS</strong> though is <strong>~18 Gb</strong> of data, well above the limits of HDMI 2.0.</p>
<p>To compare, <a href="http://en.wikipedia.org/wiki/DisplayPort">Display Port</a>.</p>
<ul>
<li>DisplayPort 1.0 and 1.1: <strong>~8 Gb</strong></li>
<li>DisplayPort 1.2: <strong>~17 Gb</strong></li>
<li>DisplayPort 1.3: <strong>~32 Gb</strong> (NEW)</li>
</ul>
<p>They‚Äôre claiming 8K and 4K@120Hz (FPS) support with the latest standard, but 18 doesn‚Äôt divide that well in to 32, so somebody has to have their numbers wrong (admittedly I did not divide mine by 1024, but 1000). Also since 8k is 4x the resolution of 4K, and the bandwidth only roughly doubled, practically speaking DisplayPort 1.3 can only support <strong>8k 8bit 30FPS</strong>. Also that 4K@120Hz is <strong>4K 8bit 120FPS</strong>. Still, if you don‚Äôt want 120FPS, that leaves room for <strong>4K 16bit 60FPS</strong>, which should be more than needed (12bit). I wonder if anybody will support <strong>4K 12bit 90FPS</strong> over DisplayPort?</p>
<p>And that‚Äôs 4K.</p>
<h2 id="1080p-and-2k-deep-color">1080p and 2K Deep Color</h2>
<p>Today 1080p is the dominant high resolution: 1920√ó1080. To the film guys, true 2K is 2048√ó1080, but there are a wide variety of devices in the same range, such as 2560√ó1600 and 2560√ó1440 (4x 720p). These, including 1080p, are often grouped under the label <a href="http://en.wikipedia.org/wiki/2K_resolution">2K</a>.</p>
<p>A second of <strong>1080p 8bit 60FPS</strong> data requires <strong>~3 Gb</strong> of bandwidth, well within the range supported by the original HDMI 1.0 spec (though why we even had to deal with 1080i is a good question, probably due to the inability to even meet the HDMI spec).</p>
<p>To compare, a second of <strong>1080p 12bit 60FPS</strong> data requires ~4.5 Gb of bandwidth. Even <strong>1080p 16bit 60FPS</strong> needed only ~6 Gb, well within the range supported by HDMI 1.3 (where Deep Color was introduced). Plenty of headroom still. Only when we push <strong>2560√ó1440 12bit 60FPS</strong> (~8 Gb) do we hit the limits of HDMI 1.3.</p>
<p>So from a specs perspective, I just wanted to note this because <strong>Deep Color</strong> and <strong>1080p</strong> are reasonable to support on now-current generation game consoles. Even the PlayStation 3, by specs, supported this. High end games probably didn‚Äôt have enough processing to spare for this, but it‚Äôs something to definitely consider supporting on PlayStation 4 and Xbox One. As for PC, many current GPUs support Deep Color in full-screen resolutions. Again, full-screen, not necessarily on your Desktop (i.e. windowed). From what I briefly read, Deep Color is only supported on the Desktop with specialty cards (FirePro, etc).</p>
<h2 id="one-more-thing-ycrcb-ycc-and-xvycc">One more thing: YCrCb (YCC) and xvYCC</h2>
<p>You make have noticed watching a video file that the blacks don‚Äôt look very black.</p>
<p>Due to a horrible legacy thing (CRT displays), data encoded as YCrCb use values from 16-&gt;240 (15-235?) instead of 0-&gt;255. Thats quite the loss, nearly 12% of the available data range, effectively lowering the precision below 8bit. The only reason it‚Äôs still done is because of old CRT televisions, which can be really tough to find these days. Regrettably, that does mean both of the original DVD and Bluray movies standards were forced to comply to this.</p>
<p>Sony proposed <a href="http://en.wikipedia.org/wiki/XvYCC">x.v.Color (xvYCC)</a> as a way of finally forgetting this stupid limitation of old CRT displays, and using the full 0-&gt;255 range. As of HDMI 1.3 (June 2006), <strong>xvYCC</strong> and <strong>Deep Color</strong> are part of the HDMI spec.</p>
<p>Several months later (November 2006), <strong>The PlayStation 3</strong> was launched. So as a rule of thumb, only HDMI devices newer than the PlayStation 3 will could potentially support xvYCC. This means televisions, audio receivers, other set top boxes, etc. It‚Äôs worth noting that some audio receivers may actually clip video signals to the 16-240 range, thus ruining picture quality of an xvYCC source. Also the PS3 was eventually updated to HDMI 1.4 via a software update, but the only 1.4 feature supported is Stereoscopic 3D.</p>
<p><a href="http://www.audioholics.com/home-theater-calibration/hdmi-black-levels-xvycc-rgb">Source</a>. <a href="http://en.wikipedia.org/wiki/XvYCC">Wikipedia</a>.</p>
<p>The point of bringing this up is to further emphasize the potential for color banding and terrible color reproduction over HDMI. An 8bit RGB framebuffer is potentially being compressed to fit within the YCbCr 16-240 range before it gets sent over HDMI. The PlayStation 3 has a setting for enabling the full color range (I forget the name used), and other new devices probably do to (unlikely named xvYCC).</p>
<p>According to Wikipedia, all of the Deep Color modes supported by HDMI 1.3 are xvYCC, as they should be.</p>
</div>
  
</article>
<footer class="page-footer">
  <nav class="pagination">
    <a class="prev" href="/2014/01/08/linux-device-input-notes/">‚Üê Prev Page | Linux Device Input Notes</a>
    <a class="next" href="/2014/01/25/c-typeof-vs-decltype/">C&#43;&#43; typeof vs decltype | Next Page ‚Üí</a>
  </nav>
</footer></main>
<footer class="footer">
  <span>&copy; 2020 <a href="https://toonormal.com/">TooNormal</a></span>
  <span>&middot;</span>
  <span>Powered by <a href="https://gohugo.io/" rel="noopener" target="_blank">HugoÔ∏èÔ∏è</a>Ô∏è</span>
</footer>
<script src="https://toonormal.com/highlight.min.js"></script>
<script>
  hljs.initHighlightingOnLoad();
</script>
</body>
</html>

